{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🖥️ Server Logs Anomaly Detection Training\n",
        "\n",
        "This notebook trains an anomaly detection model for server logs using Apache access logs data.\n",
        "\n",
        "## Features Used:\n",
        "- **response_time**: Time taken to process the request (ms)\n",
        "- **request_size**: Size of the request in bytes\n",
        "- **status_code_numeric**: HTTP status code as numeric value\n",
        "- **hour_of_day**: Hour when request was made (0-23)\n",
        "- **requests_per_minute**: Rate of requests per minute\n",
        "\n",
        "## Anomaly Types to Detect:\n",
        "- 🚨 **High response times** (potential DDoS or performance issues)\n",
        "- 🚨 **Unusual status code patterns** (4xx, 5xx errors)\n",
        "- 🚨 **Abnormal request sizes** (potential attacks)\n",
        "- 🚨 **Traffic spikes** (unusual request rates)\n",
        "- 🚨 **Off-hours activity** (requests at unusual times)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Libraries imported successfully!\n",
            "📊 Starting Server Logs Anomaly Detection Training...\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import joblib\n",
        "from datetime import datetime, timedelta\n",
        "import requests\n",
        "import io\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for plots\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"✅ Libraries imported successfully!\")\n",
        "print(\"📊 Starting Server Logs Anomaly Detection Training...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🏗️ Generating synthetic server logs dataset...\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "probabilities do not sum to 1",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 96\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m# Generate the dataset\u001b[39;00m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m🏗️ Generating synthetic server logs dataset...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m df = \u001b[43mgenerate_server_logs_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manomaly_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m📊 Dataset created:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     99\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Total samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mgenerate_server_logs_data\u001b[39m\u001b[34m(n_samples, anomaly_rate)\u001b[39m\n\u001b[32m      8\u001b[39m normal_samples = \u001b[38;5;28mint\u001b[39m(n_samples * (\u001b[32m1\u001b[39m - anomaly_rate))\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Generate normal data\u001b[39;00m\n\u001b[32m     11\u001b[39m normal_data = {\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# Response time: Normal distribution around 200ms, peak hours slower\u001b[39;00m\n\u001b[32m     13\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mresponse_time\u001b[39m\u001b[33m'\u001b[39m: np.random.gamma(\u001b[32m2\u001b[39m, \u001b[32m100\u001b[39m),  \u001b[38;5;66;03m# Gamma distribution for response times\u001b[39;00m\n\u001b[32m     14\u001b[39m \n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# Request size: Most requests small, some larger\u001b[39;00m\n\u001b[32m     16\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mrequest_size\u001b[39m\u001b[33m'\u001b[39m: np.random.lognormal(\u001b[32m8\u001b[39m, \u001b[32m1.5\u001b[39m),  \u001b[38;5;66;03m# Log-normal for request sizes\u001b[39;00m\n\u001b[32m     17\u001b[39m \n\u001b[32m     18\u001b[39m     \u001b[38;5;66;03m# Status codes: Mostly 200, some 304, few 404/500\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mstatus_code_numeric\u001b[39m\u001b[33m'\u001b[39m: \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m304\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m404\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m                                           \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnormal_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m                                           \u001b[49m\u001b[43mp\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0.75\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.75\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.75\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.75\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.75\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.75\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.08\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.02\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     22\u001b[39m \n\u001b[32m     23\u001b[39m     \u001b[38;5;66;03m# Hour of day: Peak during business hours (9-17), low at night\u001b[39;00m\n\u001b[32m     24\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mhour_of_day\u001b[39m\u001b[33m'\u001b[39m: np.concatenate([\n\u001b[32m     25\u001b[39m         np.random.choice(\u001b[38;5;28mrange\u001b[39m(\u001b[32m9\u001b[39m, \u001b[32m18\u001b[39m), size=\u001b[38;5;28mint\u001b[39m(normal_samples * \u001b[32m0.6\u001b[39m)),  \u001b[38;5;66;03m# Business hours\u001b[39;00m\n\u001b[32m     26\u001b[39m         np.random.choice(\u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[32m24\u001b[39m), size=\u001b[38;5;28mint\u001b[39m(normal_samples * \u001b[32m0.4\u001b[39m))   \u001b[38;5;66;03m# Other hours\u001b[39;00m\n\u001b[32m     27\u001b[39m     ]),\n\u001b[32m     28\u001b[39m \n\u001b[32m     29\u001b[39m     \u001b[38;5;66;03m# Requests per minute: Normal load 20-80 requests/min\u001b[39;00m\n\u001b[32m     30\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mrequests_per_minute\u001b[39m\u001b[33m'\u001b[39m: np.random.normal(\u001b[32m50\u001b[39m, \u001b[32m15\u001b[39m)\n\u001b[32m     31\u001b[39m }\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Generate anomalous data\u001b[39;00m\n\u001b[32m     34\u001b[39m anomaly_samples = n_samples - normal_samples\n",
            "\u001b[36mFile \u001b[39m\u001b[32mnumpy/random/mtrand.pyx:994\u001b[39m, in \u001b[36mnumpy.random.mtrand.RandomState.choice\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[31mValueError\u001b[39m: probabilities do not sum to 1"
          ]
        }
      ],
      "source": [
        "# Generate synthetic server logs data (since we'll simulate realistic server logs)\n",
        "def generate_server_logs_data(n_samples=50000, anomaly_rate=0.05):\n",
        "    \"\"\"Generate synthetic server logs data with realistic patterns\"\"\"\n",
        "    \n",
        "    np.random.seed(42)  # For reproducibility\n",
        "    \n",
        "    # Normal server behavior patterns\n",
        "    normal_samples = int(n_samples * (1 - anomaly_rate))\n",
        "    \n",
        "    # Generate normal data\n",
        "    normal_data = {\n",
        "        # Response time: Normal distribution around 200ms, peak hours slower\n",
        "        'response_time': np.random.gamma(2, 100),  # Gamma distribution for response times\n",
        "        \n",
        "        # Request size: Most requests small, some larger\n",
        "        'request_size': np.random.lognormal(8, 1.5),  # Log-normal for request sizes\n",
        "        \n",
        "        # Status codes: Mostly 200, some 304, few 404/500\n",
        "        'status_code_numeric': np.random.choice([200, 200, 200, 200, 200, 200, 304, 404, 500], \n",
        "                                               size=normal_samples, \n",
        "                                               p=[0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.15, 0.08, 0.02]),\n",
        "        \n",
        "        # Hour of day: Peak during business hours (9-17), low at night\n",
        "        'hour_of_day': np.concatenate([\n",
        "            np.random.choice(range(9, 18), size=int(normal_samples * 0.6)),  # Business hours\n",
        "            np.random.choice(range(0, 24), size=int(normal_samples * 0.4))   # Other hours\n",
        "        ]),\n",
        "        \n",
        "        # Requests per minute: Normal load 20-80 requests/min\n",
        "        'requests_per_minute': np.random.normal(50, 15)\n",
        "    }\n",
        "    \n",
        "    # Generate anomalous data\n",
        "    anomaly_samples = n_samples - normal_samples\n",
        "    \n",
        "    anomaly_data = {\n",
        "        # High response times (performance issues, DDoS)\n",
        "        'response_time': np.concatenate([\n",
        "            np.random.gamma(2, 100),  # Some normal\n",
        "            np.random.gamma(2, 2000)  # Some very high (anomalies)\n",
        "        ])[:anomaly_samples],\n",
        "        \n",
        "        # Unusual request sizes (potential attacks)\n",
        "        'request_size': np.concatenate([\n",
        "            np.random.lognormal(8, 1.5),      # Normal sizes\n",
        "            np.random.lognormal(12, 2),       # Very large requests (attacks)\n",
        "            np.random.lognormal(4, 1)         # Very small requests\n",
        "        ])[:anomaly_samples],\n",
        "        \n",
        "        # More error status codes in anomalies\n",
        "        'status_code_numeric': np.random.choice([200, 400, 403, 404, 500, 502, 503], \n",
        "                                               size=anomaly_samples,\n",
        "                                               p=[0.3, 0.15, 0.15, 0.15, 0.1, 0.1, 0.05]),\n",
        "        \n",
        "        # Unusual hours (night time attacks)\n",
        "        'hour_of_day': np.random.choice([0, 1, 2, 3, 4, 5, 22, 23], size=anomaly_samples),\n",
        "        \n",
        "        # Traffic spikes or unusual quiet periods\n",
        "        'requests_per_minute': np.concatenate([\n",
        "            np.random.normal(200, 50),  # Traffic spikes\n",
        "            np.random.normal(5, 2)      # Unusual quiet periods\n",
        "        ])[:anomaly_samples]\n",
        "    }\n",
        "    \n",
        "    # Combine normal and anomaly data\n",
        "    data = {\n",
        "        'response_time': np.concatenate([normal_data['response_time'], anomaly_data['response_time']]),\n",
        "        'request_size': np.concatenate([normal_data['request_size'], anomaly_data['request_size']]),\n",
        "        'status_code_numeric': np.concatenate([normal_data['status_code_numeric'], anomaly_data['status_code_numeric']]),\n",
        "        'hour_of_day': np.concatenate([normal_data['hour_of_day'], anomaly_data['hour_of_day']]),\n",
        "        'requests_per_minute': np.concatenate([normal_data['requests_per_minute'], anomaly_data['requests_per_minute']])\n",
        "    }\n",
        "    \n",
        "    # Create labels (0 = normal, 1 = anomaly)\n",
        "    labels = np.concatenate([\n",
        "        np.zeros(normal_samples),  # Normal samples\n",
        "        np.ones(anomaly_samples)   # Anomaly samples\n",
        "    ])\n",
        "    \n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "    df['is_anomaly'] = labels\n",
        "    \n",
        "    # Add some realistic constraints\n",
        "    df['response_time'] = np.clip(df['response_time'], 50, 10000)  # 50ms to 10s\n",
        "    df['request_size'] = np.clip(df['request_size'], 100, 100000)  # 100B to 100KB\n",
        "    df['requests_per_minute'] = np.clip(df['requests_per_minute'], 1, 500)  # 1 to 500 req/min\n",
        "    \n",
        "    # Shuffle the data\n",
        "    df = df.sample(frac=1).reset_index(drop=True)\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Generate the dataset\n",
        "print(\"🏗️ Generating synthetic server logs dataset...\")\n",
        "df = generate_server_logs_data(n_samples=50000, anomaly_rate=0.05)\n",
        "\n",
        "print(f\"📊 Dataset created:\")\n",
        "print(f\"   Total samples: {len(df)}\")\n",
        "print(f\"   Normal samples: {len(df[df['is_anomaly'] == 0])}\")\n",
        "print(f\"   Anomaly samples: {len(df[df['is_anomaly'] == 1])}\")\n",
        "print(f\"   Anomaly rate: {df['is_anomaly'].mean():.2%}\")\n",
        "\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exploratory Data Analysis\n",
        "print(\"📈 Performing Exploratory Data Analysis...\")\n",
        "\n",
        "# Basic statistics\n",
        "print(\"\\n📊 Dataset Statistics:\")\n",
        "print(df.describe())\n",
        "\n",
        "# Check for missing values\n",
        "print(f\"\\n🔍 Missing Values: {df.isnull().sum().sum()}\")\n",
        "\n",
        "# Feature distributions\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "fig.suptitle('🖥️ Server Logs Feature Distributions', fontsize=16, fontweight='bold')\n",
        "\n",
        "features = ['response_time', 'request_size', 'status_code_numeric', 'hour_of_day', 'requests_per_minute']\n",
        "\n",
        "for idx, feature in enumerate(features):\n",
        "    row = idx // 3\n",
        "    col = idx % 3\n",
        "    \n",
        "    # Plot distribution for normal vs anomaly\n",
        "    df[df['is_anomaly'] == 0][feature].hist(alpha=0.7, bins=50, ax=axes[row, col], \n",
        "                                           label='Normal', color='green', density=True)\n",
        "    df[df['is_anomaly'] == 1][feature].hist(alpha=0.7, bins=50, ax=axes[row, col], \n",
        "                                           label='Anomaly', color='red', density=True)\n",
        "    \n",
        "    axes[row, col].set_title(f'{feature.replace(\"_\", \" \").title()}')\n",
        "    axes[row, col].set_xlabel(feature.replace(\"_\", \" \").title())\n",
        "    axes[row, col].set_ylabel('Density')\n",
        "    axes[row, col].legend()\n",
        "\n",
        "# Remove the empty subplot\n",
        "fig.delaxes(axes[1, 2])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Correlation matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "correlation_matrix = df[features].corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
        "            square=True, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
        "plt.title('🔗 Feature Correlation Matrix')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare features for training\n",
        "print(\"🔧 Preparing features for training...\")\n",
        "\n",
        "# Select feature columns\n",
        "feature_columns = ['response_time', 'request_size', 'status_code_numeric', 'hour_of_day', 'requests_per_minute']\n",
        "X = df[feature_columns].copy()\n",
        "y = df['is_anomaly'].copy()\n",
        "\n",
        "print(f\"Features shape: {X.shape}\")\n",
        "print(f\"Features: {feature_columns}\")\n",
        "\n",
        "# Check for any infinite or NaN values\n",
        "print(f\"Infinite values: {np.isinf(X).sum().sum()}\")\n",
        "print(f\"NaN values: {X.isnull().sum().sum()}\")\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\n📊 Data Split:\")\n",
        "print(f\"   Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"   Test set: {X_test.shape[0]} samples\")\n",
        "print(f\"   Training anomaly rate: {y_train.mean():.2%}\")\n",
        "print(f\"   Test anomaly rate: {y_test.mean():.2%}\")\n",
        "\n",
        "# Display sample data\n",
        "print(f\"\\n🔍 Sample Training Data:\")\n",
        "print(X_train.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature scaling\n",
        "print(\"⚖️ Scaling features...\")\n",
        "\n",
        "# Initialize scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit scaler on training data only\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"✅ Features scaled successfully!\")\n",
        "print(f\"   Scaler mean: {scaler.mean_}\")\n",
        "print(f\"   Scaler scale: {scaler.scale_}\")\n",
        "\n",
        "# Verify scaling\n",
        "print(f\"\\n📊 Scaled Training Data Statistics:\")\n",
        "print(f\"   Mean: {X_train_scaled.mean(axis=0)}\")\n",
        "print(f\"   Std: {X_train_scaled.std(axis=0)}\")\n",
        "\n",
        "# Convert back to DataFrame for easier handling\n",
        "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=feature_columns)\n",
        "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=feature_columns)\n",
        "\n",
        "print(f\"\\n🔍 Sample Scaled Training Data:\")\n",
        "print(X_train_scaled_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Isolation Forest model\n",
        "print(\"🌲 Training Isolation Forest model...\")\n",
        "\n",
        "# For training, we only use normal data (unsupervised anomaly detection)\n",
        "X_train_normal = X_train_scaled[y_train == 0]\n",
        "print(f\"Training on {len(X_train_normal)} normal samples\")\n",
        "\n",
        "# Initialize and train Isolation Forest\n",
        "model = IsolationForest(\n",
        "    contamination=0.05,  # Expected proportion of anomalies\n",
        "    random_state=42,\n",
        "    n_estimators=100,    # Number of trees\n",
        "    max_samples=0.8,     # Fraction of samples to train each tree\n",
        "    max_features=1.0,    # Use all features\n",
        "    n_jobs=-1           # Use all CPU cores\n",
        ")\n",
        "\n",
        "# Train the model on normal data only\n",
        "model.fit(X_train_normal)\n",
        "\n",
        "print(\"✅ Model training completed!\")\n",
        "print(f\"   Model type: {type(model).__name__}\")\n",
        "print(f\"   Number of estimators: {model.n_estimators}\")\n",
        "print(f\"   Contamination rate: {model.contamination}\")\n",
        "\n",
        "# Make predictions on training set\n",
        "y_train_pred = model.predict(X_train_scaled)\n",
        "y_train_pred_binary = (y_train_pred == -1).astype(int)  # Convert -1/1 to 0/1\n",
        "\n",
        "# Make predictions on test set\n",
        "y_test_pred = model.predict(X_test_scaled)\n",
        "y_test_pred_binary = (y_test_pred == -1).astype(int)  # Convert -1/1 to 0/1\n",
        "\n",
        "print(f\"\\n📊 Training Set Predictions:\")\n",
        "print(f\"   Predicted anomalies: {y_train_pred_binary.sum()}\")\n",
        "print(f\"   Predicted normal: {len(y_train_pred_binary) - y_train_pred_binary.sum()}\")\n",
        "print(f\"   Predicted anomaly rate: {y_train_pred_binary.mean():.2%}\")\n",
        "\n",
        "print(f\"\\n📊 Test Set Predictions:\")\n",
        "print(f\"   Predicted anomalies: {y_test_pred_binary.sum()}\")\n",
        "print(f\"   Predicted normal: {len(y_test_pred_binary) - y_test_pred_binary.sum()}\")\n",
        "print(f\"   Predicted anomaly rate: {y_test_pred_binary.mean():.2%}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model evaluation\n",
        "print(\"📊 Evaluating model performance...\")\n",
        "\n",
        "# Classification report for test set\n",
        "print(\"🎯 Test Set Classification Report:\")\n",
        "print(classification_report(y_test, y_test_pred_binary, \n",
        "                          target_names=['Normal', 'Anomaly'], \n",
        "                          digits=3))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_test_pred_binary)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['Normal', 'Anomaly'], \n",
        "            yticklabels=['Normal', 'Anomaly'])\n",
        "plt.title('🎯 Confusion Matrix - Server Logs Anomaly Detection')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "# Calculate metrics\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "\n",
        "print(f\"\\n📈 Detailed Metrics:\")\n",
        "print(f\"   True Positives (TP): {tp}\")\n",
        "print(f\"   True Negatives (TN): {tn}\")\n",
        "print(f\"   False Positives (FP): {fp}\")\n",
        "print(f\"   False Negatives (FN): {fn}\")\n",
        "print(f\"   Accuracy: {accuracy:.3f}\")\n",
        "print(f\"   Precision: {precision:.3f}\")\n",
        "print(f\"   Recall: {recall:.3f}\")\n",
        "print(f\"   F1-Score: {f1_score:.3f}\")\n",
        "\n",
        "# Feature importance (based on path lengths in isolation trees)\n",
        "# Note: Isolation Forest doesn't have traditional feature importance, \n",
        "# but we can analyze which features contribute to anomaly scores\n",
        "print(f\"\\n🔍 Model Analysis:\")\n",
        "print(f\"   Decision function range: [{model.decision_function(X_test_scaled).min():.3f}, {model.decision_function(X_test_scaled).max():.3f}]\")\n",
        "print(f\"   Threshold for anomaly: < 0\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize anomaly detection results\n",
        "print(\"📊 Creating visualization of anomaly detection results...\")\n",
        "\n",
        "# Create a sample of data for visualization (to avoid overcrowding)\n",
        "sample_size = 2000\n",
        "sample_indices = np.random.choice(len(X_test), size=min(sample_size, len(X_test)), replace=False)\n",
        "X_test_sample = X_test.iloc[sample_indices]\n",
        "y_test_sample = y_test.iloc[sample_indices]\n",
        "y_pred_sample = y_test_pred_binary[sample_indices]\n",
        "\n",
        "# Plot pairwise feature comparisons\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "fig.suptitle('🖥️ Server Logs Anomaly Detection Results', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Define feature pairs for visualization\n",
        "feature_pairs = [\n",
        "    ('response_time', 'request_size'),\n",
        "    ('response_time', 'requests_per_minute'),\n",
        "    ('hour_of_day', 'status_code_numeric'),\n",
        "    ('request_size', 'requests_per_minute')\n",
        "]\n",
        "\n",
        "for idx, (feat1, feat2) in enumerate(feature_pairs):\n",
        "    row = idx // 2\n",
        "    col = idx % 2\n",
        "    \n",
        "    # True labels\n",
        "    normal_mask = y_test_sample == 0\n",
        "    anomaly_mask = y_test_sample == 1\n",
        "    \n",
        "    # Plot true normal points\n",
        "    axes[row, col].scatter(X_test_sample[normal_mask][feat1], \n",
        "                          X_test_sample[normal_mask][feat2],\n",
        "                          c='lightgreen', alpha=0.6, s=30, label='True Normal', marker='o')\n",
        "    \n",
        "    # Plot true anomalies\n",
        "    axes[row, col].scatter(X_test_sample[anomaly_mask][feat1], \n",
        "                          X_test_sample[anomaly_mask][feat2],\n",
        "                          c='red', alpha=0.8, s=50, label='True Anomaly', marker='x')\n",
        "    \n",
        "    # Highlight false positives and false negatives\n",
        "    fp_mask = (y_test_sample == 0) & (y_pred_sample == 1)\n",
        "    fn_mask = (y_test_sample == 1) & (y_pred_sample == 0)\n",
        "    \n",
        "    if fp_mask.sum() > 0:\n",
        "        axes[row, col].scatter(X_test_sample[fp_mask][feat1], \n",
        "                              X_test_sample[fp_mask][feat2],\n",
        "                              c='orange', alpha=0.8, s=60, label='False Positive', marker='s', edgecolors='black')\n",
        "    \n",
        "    if fn_mask.sum() > 0:\n",
        "        axes[row, col].scatter(X_test_sample[fn_mask][feat1], \n",
        "                              X_test_sample[fn_mask][feat2],\n",
        "                              c='purple', alpha=0.8, s=60, label='False Negative', marker='^', edgecolors='black')\n",
        "    \n",
        "    axes[row, col].set_xlabel(feat1.replace('_', ' ').title())\n",
        "    axes[row, col].set_ylabel(feat2.replace('_', ' ').title())\n",
        "    axes[row, col].set_title(f'{feat1.replace(\"_\", \" \").title()} vs {feat2.replace(\"_\", \" \").title()}')\n",
        "    axes[row, col].legend()\n",
        "    axes[row, col].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Anomaly score distribution\n",
        "plt.figure(figsize=(12, 6))\n",
        "scores = model.decision_function(X_test_scaled)\n",
        "scores_normal = scores[y_test == 0]\n",
        "scores_anomaly = scores[y_test == 1]\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(scores_normal, bins=50, alpha=0.7, label='Normal', color='green', density=True)\n",
        "plt.hist(scores_anomaly, bins=50, alpha=0.7, label='Anomaly', color='red', density=True)\n",
        "plt.axvline(x=0, color='black', linestyle='--', label='Decision Threshold')\n",
        "plt.xlabel('Anomaly Score')\n",
        "plt.ylabel('Density')\n",
        "plt.title('🎯 Anomaly Score Distribution')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# ROC-like curve using anomaly scores\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "fpr, tpr, thresholds = roc_curve(y_test, -scores)  # Negative because lower scores = more anomalous\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('📊 ROC Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"📊 ROC AUC Score: {roc_auc:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the trained model and scaler\n",
        "print(\"💾 Saving trained model and scaler...\")\n",
        "\n",
        "# Save the model\n",
        "model_filename = 'model_server_logs.joblib'\n",
        "joblib.dump(model, model_filename)\n",
        "print(f\"✅ Model saved as: {model_filename}\")\n",
        "\n",
        "# Save the scaler\n",
        "scaler_filename = 'scaler_server_logs.joblib'\n",
        "joblib.dump(scaler, scaler_filename)\n",
        "print(f\"✅ Scaler saved as: {scaler_filename}\")\n",
        "\n",
        "# Test loading the saved model\n",
        "print(\"\\n🔄 Testing saved model...\")\n",
        "loaded_model = joblib.load(model_filename)\n",
        "loaded_scaler = joblib.load(scaler_filename)\n",
        "\n",
        "# Test with a few samples\n",
        "test_samples = X_test.iloc[:5]\n",
        "print(f\"\\n🧪 Testing with {len(test_samples)} samples:\")\n",
        "print(test_samples)\n",
        "\n",
        "# Scale the test samples\n",
        "test_samples_scaled = loaded_scaler.transform(test_samples)\n",
        "\n",
        "# Make predictions\n",
        "predictions = loaded_model.predict(test_samples_scaled)\n",
        "anomaly_scores = loaded_model.decision_function(test_samples_scaled)\n",
        "\n",
        "print(f\"\\n🎯 Predictions:\")\n",
        "for i, (pred, score) in enumerate(zip(predictions, anomaly_scores)):\n",
        "    is_anomaly = pred == -1\n",
        "    print(f\"   Sample {i+1}: {'🚨 ANOMALY' if is_anomaly else '✅ Normal'} (score: {score:.3f})\")\n",
        "\n",
        "print(f\"\\n🎉 Model training and saving completed successfully!\")\n",
        "print(f\"📁 Model files created:\")\n",
        "print(f\"   - {model_filename}\")\n",
        "print(f\"   - {scaler_filename}\")\n",
        "\n",
        "# Show final model summary\n",
        "print(f\"\\n📋 Final Model Summary:\")\n",
        "print(f\"   Model Type: Isolation Forest\")\n",
        "print(f\"   Training Samples: {len(X_train_normal)}\")\n",
        "print(f\"   Features: {len(feature_columns)}\")\n",
        "print(f\"   Test Accuracy: {accuracy:.3f}\")\n",
        "print(f\"   Test Precision: {precision:.3f}\")\n",
        "print(f\"   Test Recall: {recall:.3f}\")\n",
        "print(f\"   Test F1-Score: {f1_score:.3f}\")\n",
        "print(f\"   ROC AUC: {roc_auc:.3f}\")\n",
        "print(f\"   Model is ready for deployment! 🚀\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
